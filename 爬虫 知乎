import re

import requests

# 知乎有反爬虫，加入http headers伪装浏览器
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36",
    "Connection": "keep-alive",
    "Accept": "text/html,application/json,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "zh-CN,zh;q=0.8"}

# 知乎问题id
question_id = 434923215
interval = 1
offset = 0
rank = 100
novels_count = dict()

b = re.compile(r'<.+?>')
while True:
    # 知乎获取回答分页API
    url = f'https://www.zhihu.com/api/v4/questions/{question_id}/answers?include=content&limit={interval}&offset={offset}&sort_by=default'
    html = requests.get(url, headers=headers)

    answers = html.json()['data']

    if len(answers) == 0:
        break
    for answer in answers:
        profile = answer['author']
        authorname = profile['name']
        authoranswer = answer['content']
        f = re.sub('</p>', '\n', authoranswer)
        g = re.sub(b, '', f)
        q = "回答作者：" + authorname
        print(q)
        print(g)
        print('\n')
        offset += interval
        finalfinding = q+"\n"+g+"\n"#将本答复进行格式整理，以写入文档
        # 打开一个文件
        fo = open("zhihu.txt", "a")
        fo.write(finalfinding)
        # 关闭打开的文件
        fo.close()
        break
